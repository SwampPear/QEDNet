{*0.7}
  \titlespacing*{\subsubsection}{0pt}{*1.5}{*0.5}

  % -------- Colors, links, references --------
  \usepackage[dvipsnames]{xcolor}
  \definecolor{link}{HTML}{155E95}
  \definecolor{cite}{HTML}{7A1EA1}
  \definecolor{urlc}{HTML}{0B6C4F}
  \usepackage{hyperref}
  \hypersetup{
    colorlinks=true,
    linkcolor=link,
    citecolor=cite,
    urlcolor=urlc,
    pdfauthor={Your Name},
    pdftitle={Paper Title}
  }
  \usepackage[capitalise,nameinlink]{cleveref}

  % -------- Math & symbols --------
  \usepackage{amsmath,amssymb,mathtools}
  \numberwithin{equation}{section}

  % -------- Figures, tables, captions --------
  \usepackage{graphicx}
  \usepackage{subcaption}
  \usepackage{booktabs}
  \usepackage{siunitx}
  \sisetup{detect-weight=true,detect-family=true}
  \usepackage[labelfont=bf]{caption}
  \captionsetup{font=small}

  % -------- Algorithms (choose one) --------
  \usepackage{algorithm}
  \usepackage{algpseudocode} % from algorithmicx
  \algrenewcommand\algorithmiccomment[1]{\hfill\(\triangleright\)\ #1}

  % -------- Lists & compact spacing --------
  \usepackage{enumitem}
  \setlist{nosep}

  % -------- Theorems --------
  \usepackage{amsthm}
  \newtheorem{theorem}{Theorem}
  \newtheorem{lemma}{Lemma}
  \theoremstyle{definition}
  \newtheorem{definition}{Definition}
  \theoremstyle{remark}
  \newtheorem{remark}{Remark}

  % -------- Handy commands --------
  \newcommand{\E}{\mathbb{E}}
  \newcommand{\Var}{\mathrm{Var}}
  \newcommand{\R}{\mathbb{R}}
  \newcommand{\todo}[1]{\textcolor{red}{[TODO: #1]}}

  % -------- Bibliography (natbib + BibTeX) --------
  \usepackage[numbers,sort&compress]{natbib}
  % To use a .bib file, uncomment the two lines near the end of the document
  % and create a references.bib file (example provided at bottom of this file).

  % -------- Title block --------
  \title{\\[-0.8em]\textbf{Your Catchy Paper Title}:\\[0.25em]A Concise Subtitle Reflecting the Core Contribution\\[-0.3em]}
  \author{
    First Author\thanks{Equal contribution.}\\Affiliation One \\
    \texttt{first@univ.edu}
    \and
    Second Author\footnotemark[1]\\Affiliation Two \\
    \texttt{second@lab.org}
    \and
    Senior Author\\Affiliation Three \\
    \texttt{senior@company.com}
  }
  \date{\today}

  % ========================= Document ===============================
  \begin{document}
  \maketitle

  % -------- Teaser figure (optional) --------
  % \begin{figure}[h]
  %   \centering
  %   \includegraphics[width=0.9\linewidth]{figures/teaser.pdf}
  %   \caption{A high-level schematic of the proposed method.}
  %   \label{fig:teaser}
  % \end{figure}

  % -------- Abstract --------
  \begin{abstract}
    We present a short, informative summary of the problem, the core idea, and key results. Keep it 4--6 sentences: the first explains the problem and why it matters, the next outlines your method and novelty, then one or two headline results with metrics, and a closing statement on impact and limitations.
  \end{abstract}

  % -------- Keywords (optional) --------
  \vspace{0.25em}
  \noindent\textbf{Keywords:} artificial intelligence, machine learning, deep learning, optimization

  \section{Introduction}
  Frame the problem, its significance, and prior art succinctly. State contributions clearly:
  \begin{itemize}
  \item A novel method for \emph{X} that improves \emph{Y}.
  \item A new dataset/benchmark/analysis.
  \item Strong empirical/theoretical support with ablations.
  \end{itemize}

  \section{Related Work}
  Situate your approach among core threads of literature. Use \citet{lecun2015deep} style for narrative citations or \citep{kingma2014adam} for parenthetical.

  \section{Method}
  Describe the model/algorithm with clear notation.
  \subsection{Problem Setup}
  Let $\mathcal{D}=\{(x_i,y_i)\}_{i=1}^n$ and parameters $\theta\in\R^d$. We seek to minimize
  \begin{equation}
    \label{eq:objective}
    \mathcal{L}(\theta)=\E_{(x,y)\sim\mathcal{D}}\, \ell\big(f_\theta(x),y\big)+\lambda\,\Omega(\theta).
  \end{equation}

  \subsection{Algorithm}
  \begin{algorithm}[H]
    \caption{Your Algorithm}
    \label{alg:ours}
    \begin{algorithmic}[1]
      \Require data $\mathcal{D}$, steps $T$, learning rate $\eta$
      \State initialize $\theta$
      \For{$t=1$ to $T$}
      \State sample mini-batch $B \subset \mathcal{D}$
      \State update $\theta \leftarrow \theta - \eta\, \nabla_\theta \mathcal{L}_B(\theta)$ \Comment{e.g., Adam}
      \EndFor
      \State \Return $\theta$
    \end{algorithmic}
  \end{algorithm}

  \section{Experiments}
  \subsection{Setup}
  Report datasets, baselines, metrics, and compute. Ensure reproducibility.

  \subsection{Results}
  \begin{table}[H]
    \centering
    \caption{Main benchmark results. Mean$\pm$std over three seeds.}
    \label{tab:main}
    \begin{tabular}{@{}lccc@{}}
      \toprule
      Method & Dataset A & Dataset B & Dataset C \\
      \midrule
      Baseline & 88.2 & 76.4 & 65.0 \\
      Ours & \textbf{91.0} & \textbf{79.2} & \textbf{68.1} \\
      \bottomrule
    \end{tabular}
  \end{table}

  \begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/ablation.pdf}
    \caption{Ablation of components and sensitivity to hyperparameters.}
    \label{fig:ablation}
  \end{figure}

  \section{Discussion}
  Interpret results, limitations, and ethical considerations (fairness, bias, safety, environmental impact). State responsible release plans if applicable.

  \section{Conclusion}
  Summarize the contribution, key findings, and future directions.

  % -------- Acknowledgments (optional) --------
  % \paragraph{Acknowledgments.} Funding, data providers, collaborators.

  % -------- References --------
  % Option A: Use BibTeX (.bib file) — recommended
  % Uncomment the two lines below and create references.bib
  % \bibliographystyle{unsrtnat}
  % \bibliography{references}

  % Option B: Inline sample bibliography (remove in final paper)
  \begin{thebibliography}{9}
  \bibitem{lecun2015deep}Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. \emph{Nature}, 521(7553):436–444, 2015.
  \bibitem{kingma2014adam}D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In \emph{ICLR}, 2015.
  \end{thebibliography}

  % -------------------- Appendix --------------------
  \appendix
  \section{Additional Details}
  Extended proofs, additional experiments, and qualitative results.

  \end{document}

  % ==================================================================
  % Example references.bib (copy into a new file named references.bib)
  % ==================================================================
  % @article{lecun2015deep,
  %   title={Deep learning},
  %   author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  %   journal={Nature},
  %   volume={521},
  %   number={7553},
  %   pages={436--444},
  %   year={2015}
  % }
  %
  % @inproceedings{kingma2014adam,
  %   title={Adam: A Method for Stochastic Optimization},
  %   author={Kingma, Diederik P. and Ba, Jimmy},
  %   booktitle={International Conference on Learning Representations},
  %   year={2015}
  % }
  ~
